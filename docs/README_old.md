# CellMorphNet

**Lightweight, Real-Time Blood Cell Morphology Classifier**

A compact, production-ready deep learning pipeline for classifying blood cell types from microscopic images using attention-enhanced lightweight CNNs, optimized for real-time inference on Apple M-series devices.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ¯ Project Overview

CellMorphNet demonstrates end-to-end machine learning engineering for medical image classification:

- **Data Engineering**: Multi-dataset support (BCCD, LISC, BloodMNIST) with preprocessing pipelines
- **Model Architecture**: Lightweight CNNs (EfficientNet-Lite, MobileNetV3, ResNet18) with morphology attention
- **Training**: Mixed precision (AMP), gradient accumulation, transfer learning optimized for limited hardware
- **Explainability**: Grad-CAM visualizations for interpretable predictions
- **Deployment**: CoreML export for <50ms inference on M2, plus REST API and web demo

---

## âœ¨ Key Features

- ğŸš€ **Real-time inference** on M-series Macs (<50ms per image after CoreML optimization)
- ğŸ¯ **High accuracy** (>95% on curated datasets with lightweight models)
- ğŸ” **Explainable AI** via Grad-CAM heatmaps showing model attention
- ğŸ’» **Resource-efficient** training on 8GB RAM Mac M2
- ğŸ“Š **Production-ready** with REST API, web UI, and model export tools
- ğŸ”¬ **Medical-grade** augmentations including stain normalization

---

## ğŸ—ï¸ Project Structure

```
CellMorphNet/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py                 # Dataset loaders & preprocessing
â”‚   â”œâ”€â”€ augment.py              # Albumentations pipelines
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ backbones.py        # EfficientNet, MobileNet, ResNet
â”‚   â”‚   â””â”€â”€ morph_attention.py  # Morphology attention modules
â”‚   â”œâ”€â”€ train.py                # Training pipeline with AMP
â”‚   â”œâ”€â”€ infer.py                # Inference & Grad-CAM
â”‚   â””â”€â”€ export_coreml.py        # Model export utilities
â”œâ”€â”€ demos/
â”‚   â”œâ”€â”€ streamlit_app.py        # Interactive web UI
â”‚   â””â”€â”€ fastapi_server.py       # REST API server
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_bloodmnist.py  # Dataset downloader
â”œâ”€â”€ notebooks/                  # EDA & experiments
â”œâ”€â”€ data/                       # Datasets (raw & processed)
â”œâ”€â”€ models/                     # Trained models & checkpoints
â”œâ”€â”€ results/                    # Visualizations & metrics
â”œâ”€â”€ configs/                    # Training configurations
â”œâ”€â”€ DATASETS.md                 # Dataset documentation
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone repository
git clone https://github.com/yourusername/CellMorphNet.git
cd CellMorphNet

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Download Dataset

For quick start, use BloodMNIST (lightweight subset):

```bash
python scripts/download_bloodmnist.py --sample_limit 300
```

This downloads ~888 images (296 per split) across 8 classes. For more options, see [DATASETS.md](DATASETS.md).

### 3. Train Model

**Quick training on Mac M2:**

```bash
python src/train.py \
    --data_dir data/raw/bloodmnist \
    --num_classes 8 \
    --backbone efficientnet_lite0 \
    --epochs 20 \
    --batch_size 8 \
    --accumulation_steps 4 \
    --img_size 224 \
    --lr 1e-4 \
    --output_dir models/bloodmnist_exp
```

**Advanced training with frozen backbone:**

```bash
python src/train.py \
    --data_dir data/raw/bloodmnist \
    --num_classes 8 \
    --backbone mobilenet_v3_small \
    --epochs 30 \
    --batch_size 16 \
    --freeze_backbone \
    --unfreeze_after 5 \
    --use_class_weights \
    --img_size 224
```

Training takes ~10-30 minutes on M2 for lightweight models.

### 4. Run Inference

**Single image with Grad-CAM:**

```bash
python src/infer.py \
    --checkpoint models/bloodmnist_exp/checkpoints/best.pth \
    --image data/raw/bloodmnist/test/basophil/basophil_0000.png \
    --gradcam
```

**Batch inference:**

```bash
python src/infer.py \
    --checkpoint models/bloodmnist_exp/checkpoints/best.pth \
    --image_dir data/raw/bloodmnist/test/basophil/ \
    --output_dir results/predictions \
    --gradcam
```

### 5. Export to CoreML

```bash
python src/export_coreml.py \
    --checkpoint models/bloodmnist_exp/checkpoints/best.pth \
    --output_dir models/exported \
    --all \
    --quantize \
    --benchmark
```

This exports to TorchScript, ONNX, and CoreML formats, with optional FP16 quantization.

### 6. Launch Demo

**Streamlit Web UI:**

```bash
streamlit run demos/streamlit_app.py
```

Access at http://localhost:8501

**FastAPI REST Server:**

```bash
cd demos
python fastapi_server.py
```

API docs at http://localhost:8000/docs

---

## ğŸ“Š Datasets

CellMorphNet supports three public datasets:

| Dataset | Size | Classes | Best For |
|---------|------|---------|----------|
| **BloodMNIST** | ~17K | 8 types | Quick experiments |
| **BCCD** | ~300 | 3 types | Detection tasks |
| **LISC** | Hundreds | 6 WBC types | High-quality classification |

**Current Status:**
- âœ… BloodMNIST: Downloaded (296 samples per split)
- âœ… BCCD: Available in `archive (1)/BCCD/`
- âœ… LISC: Available in `LISC Database/`

See [DATASETS.md](DATASETS.md) for detailed download instructions and preprocessing guides.

---

## ğŸ“ Model Architectures

### Supported Backbones

- **EfficientNet-Lite0**: ~4M params, best accuracy/speed tradeoff
- **MobileNetV3-Small**: ~2.5M params, fastest inference
- **MobileNetV3-Large**: ~5M params, higher capacity
- **ResNet18**: ~11M params, strong baseline
- **ResNet34**: ~21M params, for larger datasets

### Morphology Attention Module

Custom attention mechanism combining:
- **Texture branch**: Fine-grained appearance features (3Ã—3 conv)
- **Shape branch**: Geometric features (3Ã—3 dilated conv)
- **CBAM attention**: Channel and spatial attention
- **Residual connection**: Stable training

### Training Features

- âœ… Transfer learning from ImageNet
- âœ… Mixed precision training (AMP)
- âœ… Gradient accumulation for effective larger batches
- âœ… Progressive unfreezing of backbone layers
- âœ… Class weights for imbalanced datasets
- âœ… Cosine annealing LR schedule
- âœ… Comprehensive metrics (accuracy, F1, per-class precision/recall)

---

## ğŸ”¬ Explainability

### Grad-CAM Visualizations

Generate attention heatmaps showing which image regions influenced predictions:

```python
from src.infer import GradCAM, get_target_layer

# Load model and get target layer
target_layer = get_target_layer(model, backbone='efficientnet_lite0')
gradcam = GradCAM(model, target_layer)

# Generate heatmap
heatmap = gradcam.generate(image_tensor, target_class=2)
overlayed = gradcam.overlay_heatmap(heatmap, original_image)
```

---

## ğŸš¢ Deployment

### CoreML Export (M-series Macs)

```bash
python src/export_coreml.py \
    --checkpoint models/bloodmnist_exp/checkpoints/best.pth \
    --coreml \
    --quantize \
    --benchmark
```

**Expected Performance:**
- Model size: ~5-15 MB (FP16 quantized)
- Inference time: <50ms on M2
- Throughput: >20 images/second

### REST API

Start server:

```bash
python demos/fastapi_server.py
```

Make predictions:

```bash
curl -X POST "http://localhost:8000/predict" \
    -H "accept: application/json" \
    -H "Content-Type: multipart/form-data" \
    -F "file=@test_image.png"
```

Response:

```json
{
  "predicted_class": "neutrophil",
  "confidence": 0.97,
  "probabilities": {
    "basophil": 0.01,
    "eosinophil": 0.00,
    "neutrophil": 0.97,
    ...
  },
  "success": true
}
```

### Streamlit Demo

Interactive web interface with:
- Image upload
- Real-time classification
- Grad-CAM visualization
- Probability distribution charts
- Confidence threshold controls

---

## ğŸ“ˆ Expected Results

### Performance Benchmarks

| Model | Params | Val Accuracy | Val F1 | Inference (M2) |
|-------|--------|--------------|--------|----------------|
| EfficientNet-Lite0 | 4.0M | >95% | >0.94 | ~30ms |
| MobileNetV3-Small | 2.5M | >93% | >0.92 | ~20ms |
| ResNet18 | 11.2M | >96% | >0.95 | ~40ms |

*Results on BloodMNIST validation set after 20 epochs with default hyperparameters.*

### Training on Mac M2 8GB

- **EfficientNet-Lite0**: Batch size 8, ~2 min/epoch, 20 epochs in 40 min
- **MobileNetV3-Small**: Batch size 16, ~1.5 min/epoch, 20 epochs in 30 min
- Memory usage: ~4-6 GB

For larger models or datasets, use gradient accumulation or train on Colab/Kaggle.

---

## ğŸ› ï¸ Development

### Training Configuration

Create custom config files in `configs/`:

```yaml
# configs/experiment1.yaml
data_dir: data/raw/bloodmnist
output_dir: models/experiment1

model:
  backbone: efficientnet_lite0
  num_classes: 8
  pretrained: true
  freeze_backbone: true
  unfreeze_after: 3

training:
  epochs: 30
  batch_size: 16
  accumulation_steps: 2
  lr: 1e-4
  weight_decay: 1e-4
  optimizer: adamw
  scheduler: cosine

data:
  img_size: 224
  use_class_weights: true
  use_albumentations: true
```

### Custom Dataset

Organize your data in ImageFolder format:

```
custom_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ class1/
â”‚   â”‚   â”œâ”€â”€ img1.png
â”‚   â”‚   â””â”€â”€ img2.png
â”‚   â””â”€â”€ class2/
â”‚       â””â”€â”€ img3.png
â”œâ”€â”€ val/
â”‚   â””â”€â”€ ...
â””â”€â”€ test/
    â””â”€â”€ ...
```

Then train:

```bash
python src/train.py \
    --data_dir custom_dataset \
    --num_classes 2 \
    --epochs 50
```

---

## ğŸ“ Citation

If you use CellMorphNet in your research, please cite the original dataset sources:

### Datasets

- **BloodMNIST**: Yang et al., "MedMNIST v2", Scientific Data 2023
- **BCCD**: Shenggan, "BCCD Dataset", GitHub 2017
- **LISC**: Monash University VL4AI Lab

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- [ ] Add more backbone architectures (ViT, ConvNeXt)
- [ ] Implement self-supervised pretraining
- [ ] Add segmentation capabilities
- [ ] Create iOS/macOS native app
- [ ] Add more augmentation strategies
- [ ] Improve documentation and tutorials

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

**Dataset Licenses:**
- BCCD: Check repository
- LISC: Academic use
- BloodMNIST: CC BY 4.0

**Important:** This is a research/educational project. Not intended for clinical use without proper validation and regulatory approval.

---

## ğŸ™ Acknowledgments

- [MedMNIST](https://medmnist.com/) for standardized medical imaging benchmarks
- [BCCD Dataset](https://github.com/Shenggan/BCCD_Dataset) contributors
- [LISC Dataset](https://vl4ai.erc.monash.edu/) from Monash University
- PyTorch and torchvision teams
- Albumentations library developers

---

## ğŸ“§ Contact

**Author**: [Your Name]
**Email**: your.email@example.com
**GitHub**: [@yourusername](https://github.com/yourusername)

For questions or issues, please open a GitHub issue.

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­

---

Made with â¤ï¸ for advancing medical AI research
